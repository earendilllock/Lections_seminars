{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain Monte-Carlo Methods: Gibbs Sampling and the Metropolis-Hastings Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Introduction to Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The Metropolis-Hastings Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Markov Chain Monte Carlo (MCMC)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Markov Chain**: a stochastic process in which future states are independent of past states given the present state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Monte Carlo**: simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basically a fancy way of saying we can take quantities of interest of a distribution from simulated draws from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have a distribution $p(\\theta)$ (perhaps a posterior) that we want to take quantities of interest from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To derive it analitically, we need to take integrals:\n",
    "$$\n",
    "\\begin{equation}\n",
    "I = \\int_{\\Theta} g(\\theta) p(\\theta) d \\theta\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $g(\\theta)$ is some function of $\\theta $ ($g(\\theta) = \\theta$ for the mean and $g(\\theta) = (\\theta - E(\\theta))^2$ for the variance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can approximate the integrals via Monte Carlo Integration by simulating $M$ values from $p(\\theta)$ and calculating \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat I_M = \\frac{1}{M}\\sum_{i=1}^M g(\\theta^{(i)})\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We know that $\\hat I_M \\rightarrow I$ as $M \\rightarrow \\infty$.\n",
    "\n",
    "And we know that from the Strong Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strong Law of Large Numbers (SLLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $X_1, X_2, \\ldots$ be a sequence of **independent** and identically distributed random variables, each having a finite mean $\\mu = E(X_i)$.\n",
    "\n",
    "Then with probability 1,\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\sum_{i=1}^M X_i}{M} \\rightarrow \\mu\n",
    "\\end{equation}, \\ as \\ M \\rightarrow \\infty\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This also works with variances and other quantities of interest, since a function of i.i.d. random variables are also i.i.d. random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "But what if we canâ€™t generate draws that are **independent**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we want to draw from our posterior distribution $p(\\theta|y)$, but we cannot sample independent draws from it.\n",
    "\n",
    "For example, we often do not know the normalizing constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, we may be able to sample draws from $p(\\theta|y)$ that are slightly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we can sample slightly dependent draws using a **Markov chain**, then we can still find quantities of interests from those draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Markov Chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Definition: a *stochastic process* in which future states are independent of past states given the present state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stochastic process: a *consecutive* set of random (not deterministic) quantities defined on some known state space $\\Theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- think of $\\Theta$ as a parameter space.\n",
    "- consecurive implies a time component, indexed by $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider a draw $\\theta^t$ to be a state at iteration $t$. The next draw $\\theta^{t+1}$ is dependent only on the current draw $\\theta^t$, and not on any past draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This satisfies the **Markov property**:\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\theta^{t+1}|\\theta^t, \\ldots, \\theta^1) = p(\\theta^{t+1}|\\theta^t)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So our Markov chain is a bunch of draws of $\\theta$ that are each slightly dependent on the previous one. The chain wanders around the parameter space, remembering only where it has been in the last period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are the rules governing how the chain jumps from one state to another at each period?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The jumping rules are governed by a **transition kernel**, which is a mechanism that describes the probability of moving to some other state based on the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transition kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For discrete state space (k possible states): a $k \\times k$ matrix of transition probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each element of such matrix is defined as follows:\n",
    "$$\n",
    "\\begin{equation}\n",
    "P_{i,j} = p(\\theta^{t+1} = i   |  \\theta^t = j)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does a Markov Chain work? (Discrete Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define a starting distribution $\\pi^0 \\in \\mathbb R^{1 \\times k}$\n",
    "\n",
    "2) At iteration 1, the distribution $\\pi^1$ (from which $\\theta^1$ is drawn) is : $$\\pi^1 = \\pi^0 P$$\n",
    "\n",
    "3) At iteration $t$, the distribution $\\pi^t$ is \n",
    "$$\n",
    "\\pi^t = \\pi^{t-1} P = \\pi^0 P^t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stationary (Limiting) Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define a stationary distribution $\\pi$ to be some distribution $\\Pi$ such that $ \\pi = \\pi P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For all the MCMC algorithms we use in Bayesian statistics, the markov chain will typically **converge** to $\\pi$ regardless of our starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can decise a Markov chain whose stationary distribution $\\pi$ is our desired posterior distribution $p(\\theta|y)$, then we can run this chain to get draws that are approximately from $p(\\theta|y)$ once the chain has converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Burn-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since convergence usually occurs regardless of our starting point, we can usually pick any feasible (for example, picking starting draws that are in the parameter space) starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "However, the time it takes for the chain to converge varies depending on the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a matter of practice, most people throw out a certain number of the first draws, known as the **burn-in**. This is to make our draws closer to the stationary distribution and less dependent on the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, it is unclear how much we should burn-in since our draws are all slightly dependent and we donâ€™t know exactly when convergence occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo Integration on the Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we have a Markov chain that has converged to the stationary distribution, then the draws in our chain appear to be like draws from $p(\\theta|y)$,so it seems like we should be able to use Monte Carlo Integration methods to find quantities of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One problem: our draws are not independent, which we required for Monte Carlo Integration to work (remember SLLN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Luckily, we have the **Ergodic Theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ergodic Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\theta^1, \\theta^2, \\ldots, \\theta^M$ be $M$ values from a Markov chain that is *aperiodic, irreducible, and positive recurrent* (then the chais is ergodic) and $E[g(\\theta)] < \\infty$.\n",
    "\n",
    "Then with probability 1,\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{1}{M} \\sum_{i=1}^M g(\\theta_i) \\rightarrow \\int_{\\Theta}g(\\theta)\\pi(\\theta) d\\theta\n",
    "\\end{equation}\n",
    "$$\n",
    "as $M \\rightarrow \\infty$, where $\\pi$ is the stationary distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is the Markov chain analog of the SLLN, and it allows us to ignore the dependence between draws of the Markov chain when we calculate quantities of interest from the draws.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But what does it mean for a chain to be *aperiodic, irreducible, and positive recurrent*, and therefore **ergodic**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aperiodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Markov chain is **aperiodic** if the only length of time for which the chain repeats some cycle of values is the trivial case with cycle length equal to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let A, B, and C denote the states (analogous to the possible values of $\\theta$) in a 3-state Markov chain. The following chain is *periodic* with period 3, where the period is the number of steps that it takes to return to a certain state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"Aperiodicity.png\"  style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As long as the chain is not repeating an identical cycle, then the chain is **aperiodic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Irreducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Markov chain is **irreducible** if it is possible go from any state to any other state (not necessarily in one step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following chain is reducible, or not irreducible.\n",
    "\n",
    "<img src=\"irred.png\"  style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The chain is not irreducible because we cannot get to A from B or C regardless of the number of steps we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positive Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Markov chain is *recurrent* if for any given state $i$, if the chain starts at $i$, it will eventually return to $i$ with probability 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Markov chain is *positive recurrent* if the expected return time to state $i$ is finite; otherwise it is *null* recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So if our Markov chain is **aperiodic, irreducible,** and **positive recurrent** (all the ones we use in Bayesian statistics usually are), then it is ergodic and the ergodic theorem allows us to do Monte Carlo Integration by calculating quantities of interest from our draws, ignoring the dependence between draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thinning the Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to break the dependence between draws in the Markov chain, some have suggested only keeping every $d$-th draw of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is known as **thinning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pros:\n",
    "- Perhaps gets you a little closer to i.i.d draws\n",
    "- Saves memory since you only store a fraction of the draws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cons:\n",
    "- Unnecessary with ergodic theorem.\n",
    "- Shown to increase the variance of your Monte Carlo estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MCMC is a class of methods in which we can simulate draws that are slightly dependent and are approximately from a (posterior) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then take those draws and calculate quantities of interest for the (posterior) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Bayesian statistics, there are generally two MCMC algorithms that we use: the Gibbs Sampler and the Metropolis-Hastings algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have a joint distribution $p(\\theta_1, \\ldots, \\theta_k)$ that we want to sample from (for example, a posterior distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 100%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use the Gibbs sampler to sample from the joint distribution if we knew the **full conditional** distributions for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each parameter, the **full conditional** distribution is the distribution of the parameter conditional on the known information and all the other parameters: $p(\\theta_j|\\theta_{-j},y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we know the joint distribution simply by knowing the full conditional distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Hammersley-Clifford Theorem (for two blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have a joint density $f(x,y)$. THe theorem proves that we can write out the joint density in terms of the conditional densities $f(x|y)$ and $f(y|x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "f(x,y) = \\frac{f(y|x)}{\\int \\frac{f(y|x)}{f(x|y)} dy}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write the denominator as \n",
    "$$\n",
    "\\int \\frac{f(y|x)}{f(x|y)}dy = \\int \\frac{\\frac{f(x,y}{f(x)}}{\\frac{f(x,y)}{f(y)}}dy = \\int \\frac{f(y)}{f(x)}dy =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "= \\frac{1}{f(x)}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus. our right-hand side is \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{f(y|x)}{\\frac{1}{f(x)}} = f(y|x)f(x) = f(x,y)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The theorem shows that knowledge of the conditional densities allows us to get the joint density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This works for more than two blocks of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But how do we figure out the full conditionals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps to Calculating Full Conditional Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have a posterior $p(\\theta|y)$. To calculate the full conditionals for each $\\theta$, do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Write out the full posterior ignoring constants of proportionality.\n",
    "2. Pick a block of parameters (for example, $\\theta_1$) and drop everything that doesnâ€™t depend on $\\theta_1$.\n",
    "3. Use your knowledge of distributions to figure out what the normalizing constant is (and thus what the full conditional distribution $p(\\theta_1|\\theta_{-1}, y)$ is).\n",
    "4. Repeat steps 2 and 3 for all parameter blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampler Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Letâ€™s suppose that we are interested in sampling from the posterior $p(\\theta|y)$, where $\\theta$ is a vector of three parameters, $\\theta_1, \\theta_2, \\theta_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The steps to a Gibbs Sampler (and the analogous steps in the MCMC process) are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Pick a vector of starting values $\\theta^0$. (Define a starting distribution $\\pi^0$ and drawing $\\theta^0$ from it).\n",
    "2. Start with any $\\theta$ (order does not matter, but Iâ€™ll start with $\\theta_1$ for convenience). Draw a value $\\theta_1^1$ from the full conditional\n",
    "$p(\\theta_1|\\theta_2^0, \\theta^0_3, y)$.\n",
    "3. Draw a value $\\theta_2^1$ (again order does not matter) from the full \n",
    "conditional $p(\\theta_2| \\theta_1^1, \\theta_3^0, y)$. Note that we must use the updated value of $\\theta_1^1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "3. Draw a value $\\theta_2^1$ (again order does not matter) from the full \n",
    "conditional $p(\\theta_2| \\theta_1^1, \\theta_3^0, y)$. Note that we must use the updated value of $\\theta_1^1$\n",
    "4. Draw a value $\\theta_3^1$ from the full conditional $p(\\theta_3| \\theta_1^1, \\theta_2^1,y)$ using both updated values. (Steps 2-4 are analogous to multiplying $\\pi^0$ and $P$ to get $\\pi^1$ and then drawing $\\theta^1$ from $\\pi^1$.)\n",
    "5. Draw $\\theta^2$ using $\\theta^1$ and continually using the most updated values.\n",
    "6. Repeat until we get $M$ draws, with each draw being a vector $\\theta^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "7. Optional burn-in and/or thinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our result is a Markov chain with a bunch of draws of $\\theta$ that are approximately from our posterior. We can do Monte Carlo Integration on those draws to get quantities of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Example (Robert and Casella, 10.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have data of the number of failures $(y_i )$ for each of 10\n",
    "pumps in a nuclear plant.\n",
    "\n",
    "We also have the times $(t_i )$ at which each pump was observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to model the number of failures with a Poisson likelihood, where the expected number of failure $\\lambda_i$ differs for each pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the time which we observed each pump is different, we need to scale each $\\lambda_i$ by its observed time $t_i$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our likelihood is $\\prod_{i=1}^{10}Poisson(\\lambda_it_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's put a $Gamma(\\alpha, \\beta)$ prior on $\\lambda_i$ with $\\alpha = 1.8$, so the $\\lambda_i$ are drawn from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also. let's put a $Gamma(\\gamma, \\delta)$ prior on $\\beta$, with $\\gamma =0.01$ and $\\delta = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So our model has 11 parameters that are unknown (10 $\\lambda_i$s and $\\beta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our posterior is \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda, \\beta|y,t) &\\propto \\left( \\prod_{i=1}^{10} Poisson(\\lambda_i t_i) \\times Gamma(\\alpha, \\beta) \\right) \\times Gamma(\\gamma, \\delta)\\\\\n",
    "&= \\left(\\prod_{i=1}^{10} \\frac{\\exp^{-\\lambda_it_i}(\\lambda_it_i)^{y_i}}{y_i!} \\times \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}_i \\exp^{\\beta \\lambda_i} \\right) \\\\\n",
    "& \\times \\frac{\\delta^{\\gamma}}{\\Gamma(\\gamma)} \\beta^{\\gamma-1}\\exp^{-\\delta \\beta}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda, \\beta|y,t) &\\propto \\left(\\prod_{i=1}^{10} \\exp^{-\\lambda_it_i}(\\lambda_it_i)^{y_i} \\times \\beta^{\\alpha}\\lambda^{\\alpha-1}_i \\exp^{\\beta \\lambda_i} \\right) \\times \\beta^{\\gamma-1}\\exp^{-\\delta \\beta}\\\\\n",
    "& \\left( \\prod_{i=1}^{10}\\lambda_i^{y_i+\\alpha-1}\\exp^{-(t_i+\\beta)\\lambda_i}\\right) \\beta^{10\\alpha +\\gamma-1}\\exp^{-\\delta\\beta}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finding the full conditionals:\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\lambda_i|\\lambda_{-i}, \\beta,y,t) \\propto \\lambda_i^{y_i +\\alpha-1}\\exp^{-(t_i+\\beta)\\lambda_i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\beta|\\lambda, y,t) \\propto \\exp^{-\\beta(\\delta+ \\sum_{i=1}^{10}\\lambda_i)} \\beta^{10\\alpha+\\gamma -1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(\\lambda_i|\\lambda_{-i}, \\beta,y,t)$ is a $Gamma(y_i+\\alpha, t_i+\\beta)$ distribution.\n",
    "\n",
    "$p(\\beta|\\lambda, y,t)$ is a $Gamma(10\\alpha+\\gamma, \\delta+ \\sum_{i=1}^{10}\\lambda_i)$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding the Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Define starting values for $\\beta$ (we only need to define $\\beta$ here because we will draw $\\lambda$ first and it only depends on $\\beta$ and other given values).\n",
    "2. Draw $\\lambda^1$ from its full conditional (we're drawing all the $\\lambda_i$ as a block because they all only depends on $\\beta$ and not each other).\n",
    "3. Draw $\\beta^1$ from its full conditional, using $\\lambda^1$\n",
    "4. Repeat using most updated values until we get M draws.\n",
    "5. Optional burn-in and thinning.\n",
    "6. Make it into a function.\n",
    "7. Do Monte Carlo Integration on the resulting Markov chain, which are samples approximately from the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PyMC is an open source Python library for Bayesian learning of general Probabilistic Graphical Model\n",
    "with advanced features and easy to use interface. https://github.com/pymc-devs/pymc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Darren Wilkinson. https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Metropolis-Hastings Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have a posterior $p(\\theta|y)$ that we want to sample from, but\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the posterior doesnâ€™t look like any distribution we know (no conjugacy)\n",
    "- the posterior consists of more than 2 parameters (grid approximations intractable)\n",
    "- some (or all) of the full conditionals do not look like any distributions we know (no Gibbs sampling for those whose full conditionals we donâ€™t know)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If all else fails, we can use the **Metropolis-Hastings** algorithm, which will always work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Metropolis-Hastings Algorithm follows the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a starting value $\\theta^0$\n",
    "2. At itertion $t$, draw a candiate $\\theta^*$ from a jumping distribution $J_t(\\theta^*|\\theta^{y-1})$\n",
    "3. Compute an acceptance ratio (probability): \n",
    "$$\n",
    "\\begin{equation}\n",
    "r = \\frac{p(\\theta^* | y)/J_t(\\theta^*|\\theta^{t-1})}{p(\\theta^{t-1}|y)/J_t(\\theta^{t-1} | \\theta^*)}\n",
    "\\end{equation}\n",
    "$$\n",
    "4. Accept $\\theta^*$ as $\\theta^t$ with probability $\\min(r,1)$. If $\\theta^*$ is not accepted, then $\\theta^t = \\theta^{t-1}$\n",
    "5. Repeat steps 2-4 M times to get M draws from $p(\\theta|y)$ with optional burn-in and/or thinning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Choose a starting value $\\theta^0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to drawing from our initial stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The important thing to remember is that $\\theta^0$ must have positive probability.\n",
    "$$\n",
    "p(\\theta^0|y)>0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise, we are starting with a value that cannot be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Draw $\\theta^*$ from $J_t(\\theta^*|\\theta^{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The jumping distribution $J_t(\\theta^*|\\theta^{t-1})$ determines where we move to in the next iteration of the Markov chain (analogous to the transition kernel). The support of the jumping distribution must contain the support of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The original Metropolis algorithm required that $J_t(\\theta^*|\\theta^{t-1})$ be a symmetric distribution (such as the normal distribution), that is\n",
    "$$\n",
    "\\begin{equation}\n",
    "J_t(\\theta^*|\\theta^{t-1}) = J_t(\\theta^{t-1}|\\theta^*)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now know with the Metropolis-Hastings algorithm that symmetry is unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we have a symmetric jumping distribution that is dependent on $\\theta^{t-1}$, then we have what is known as **random walk Metropolis sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If our jumping distribution does not depend on $\\theta^{t-1}$,\n",
    "$$\n",
    "\\begin{equation}\n",
    "J_t(\\theta^*|\\theta^{t-1}) = J_t(\\theta^*)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "then we have what is known as **independent Metropolis-Hastings sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basically all our candidate draws $\\theta^*$ are drawn from the same distribution, regardless of where the previous draw was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be extremely efficient or extremely inefficient, depending on how close the jumping distribution is to the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally speaking, chain will behave well only if the jumping distribution has heavier tails than the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute acceptance ratio r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "r = \\frac{p(\\theta^* | y)/J_t(\\theta^*|\\theta^{t-1})}{p(\\theta^{t-1}|y)/J_t(\\theta^{t-1} | \\theta^*)}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the case where our jumping distribution is symmetric,\n",
    "$$\n",
    "\\begin{equation}\n",
    "r = \\frac{p(\\theta^* | y)}{p(\\theta^{t-1}|y)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If our candidate draw has higher probability than our current draw, then our candidate is better so we definitely accept it. Otherwise, our candidate is accepted according to the ratio of the probabilities of the candidate and current draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that since $r$ is a ratio, we only need $p(\\theta|y)$ up to a constant of proportionality since $p(y)$ cancels out in both the numerator and denominator.\n",
    "ï¿¼ï¿¼\n",
    "ï¿¼ï¿¼ï¿¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the case where our jumping distribution is not symmetric,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "r = \\frac{p(\\theta^* | y)/J_t(\\theta^*|\\theta^{t-1})}{p(\\theta^{t-1}|y)/J_t(\\theta^{t-1} | \\theta^*)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We need to weight our evaluations of the draws at the posterior densities by how likely we are to draw each draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, if we are very likely to jump to some $\\theta^*$, then $J_t(\\theta^*|\\theta^{t-1})$ is likely to be high, so we should accept less of them than some other $\\theta^*$ that we are less likely to jump to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the case of independent Metropolis-Hastings sampling,\n",
    "$$\n",
    "\\begin{equation}\n",
    "r = \\frac{p(\\theta^* | y)/J_t(\\theta^*)}{p(\\theta^{t-1}|y)/J_t(\\theta^{t-1})}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 4: Decide whether to accept $\\theta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Accept $\\theta^*$ as $\\theta^t$ with probability $\\min(r,1)$. If $\\theta^*$ is not accepted, then $\\theta^t = \\theta^{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. For each $\\theta^*$, draw a value $u$ from the Uniform(0,1) distribution\n",
    "2. If $u \\leq r$, accept $\\theta^*$ as $\\theta^t$. Otherwise, use $\\theta^{t-1}$ as $\\theta^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Candidate draws with higher density than the current draw are always accepted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike in rejection sampling, each iteration always produces a draw, either $\\theta^*$ or $\\theta^{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acceptance Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is important to monitor the acceptance rate (the fraction of candidate draws that are accepted) of your Metropolis-Hastings algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If your acceptance rate is too high, the chain is probably not mixing well (not moving around the parameter space quickly enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If your acceptance rate is too low, your algorithm is too inefficient (rejecting too many candidate draws)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is too high and too low depends on your specific algorithm, but generally\n",
    "- random walk: somewhere between 0.25 and 0.50 is recommended\n",
    "- independent: something close to 1 is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "http://python4mpia.github.io/fitting_data/Metropolis-Hastings.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Darren Wilkinson. https://darrenjw.wordpress.com/2010/08/15/metropolis-hastings-mcmc-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## End of lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Please, ask any questions =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
